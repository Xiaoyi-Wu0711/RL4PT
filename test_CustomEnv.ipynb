{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import os, time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from time import sleep\n",
    "from collections import namedtuple\n",
    "import os, time\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import NormalizeObservation\n",
    "import Custom_Envs\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "from stable_baselines3 import PPO, DDPG, TD3\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490.11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 194.84 + 152.94+179.99+179.98 +151.54 * 3 +327.74\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.07999999999993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a -b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347.03"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= 194.84 + 152.94+ 179.99+169.99+151.54+169.99+327.74\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Custom_Envs/CommuteEnv-v4'\n",
    "one_dim = True\n",
    "best_toll_initialization = False\n",
    "reward_shifting = True\n",
    "train_episode =  3\n",
    "simulation_day_num = 3 # the simulation days in one iteration \n",
    "train_time_steps = int(simulation_day_num * train_episode)# total train times\n",
    "save_episode_train = 2 # the episode number to save train results\n",
    "learning_rate = 0.001\n",
    "evaluation_time_episode = 2\n",
    "save_episode_test = evaluation_time_episode # the episode number to save train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " save_dir  ./results/PPO_Wed Sep 20 18:21:47 2023\n"
     ]
    }
   ],
   "source": [
    "# Dont need change parameters\n",
    "start_time = time.time()\n",
    "save_dir = \"./results/PPO_\"+time.asctime(time.localtime(start_time))\n",
    "print(\" save_dir \", save_dir)\n",
    "env_name = 'Custom_Envs/CommuteEnv-v4'\n",
    "isExist = os.path.exists(save_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment initialization\n",
    "env = gym.make(env_name, simulation_day_num = simulation_day_num, \n",
    "                save_episode_freq = save_episode_test, \n",
    "                train=False, save_dir = save_dir, space_shape=(4, int(12*60/5)),\n",
    "                 One_dim = one_dim, \n",
    "                 Best_toll_initialization = best_toll_initialization, \n",
    "                 Reward_shifting = reward_shifting)\n",
    "env = NormalizeObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiawu/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:175: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed=58287\u001b[0m\n",
      "  logger.warn(\n",
      "/home/xiawu/py38/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:244: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m check_env(env)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:444\u001b[0m, in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    439\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    440\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYour action space has dtype \u001b[39m\u001b[39m{\u001b[39;00maction_space\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m, we recommend using np.float32 to avoid cast errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         )\n\u001b[1;32m    443\u001b[0m \u001b[39m# ============ Check the returned values ===============\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m _check_returned_values(env, observation_space, action_space)\n\u001b[1;32m    446\u001b[0m \u001b[39m# ==== Check the render method and the declared render modes ====\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_render_check:\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/env_checker.py:285\u001b[0m, in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39m# Sample a random action\u001b[39;00m\n\u001b[1;32m    284\u001b[0m action \u001b[39m=\u001b[39m action_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m--> 285\u001b[0m data \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    287\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m, (\n\u001b[1;32m    288\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe `step()` method must return five values: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobs, reward, terminated, truncated, info. Actual: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m values returned.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    292\u001b[0m \u001b[39m# Unpack\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/normalize.py:81\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_vector_env:\n\u001b[1;32m     83\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(obs)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step:\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    209\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    210\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    211\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1117\u001b[0m, in \u001b[0;36mCommuteEnv_testing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1114\u001b[0m state_encode, vehicle_information, market_price, pt_share_number, sw  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msim\u001b[39m.\u001b[39mRL_simulateOneday(tollparameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday, state_aggravate) \u001b[39m# 5 days social welfare\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m observation \u001b[39m=\u001b[39m state_encode\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspace_shape)\n\u001b[0;32m-> 1117\u001b[0m AITT_daily \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# average system travel time per day is 34min, 31min\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_shifting:\n\u001b[1;32m   1119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:799\u001b[0m, in \u001b[0;36mSimulation.RL_simulateOneday\u001b[0;34m(self, tollparameters, day, state_aggravate)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mPTCs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mPTCs\n\u001b[1;32m    797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mAR\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mAR\n\u001b[0;32m--> 799\u001b[0m vehicle_information, time_list, Accumulation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMFD(day)\t\t\n\u001b[1;32m    800\u001b[0m actualArrival \u001b[39m=\u001b[39m vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_arr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m#update actual travel time\u001b[39;00m\n\u001b[1;32m    801\u001b[0m travel_time \u001b[39m=\u001b[39m  vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# daily travel time without PT\u001b[39;00m\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:733\u001b[0m, in \u001b[0;36mSimulation.MFD\u001b[0;34m(self, day)\u001b[0m\n\u001b[1;32m    729\u001b[0m vehicle_index\u001b[39m.\u001b[39mremove(Event_index)\n\u001b[1;32m    731\u001b[0m \u001b[39m# Update the predicted arrival time\u001b[39;00m\n\u001b[1;32m    732\u001b[0m travel_started_vehicles \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere((S_Event_list_array[:, \u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m&\u001b[39m (\n\u001b[0;32m--> 733\u001b[0m     np\u001b[39m.\u001b[39;49misin(S_Event_list_array[:, \u001b[39m0\u001b[39;49m], vehicle_index)))\n\u001b[1;32m    734\u001b[0m temp \u001b[39m=\u001b[39m S_Event_list_array[(travel_started_vehicles)][:, \u001b[39m0\u001b[39m]\n\u001b[1;32m    735\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msize(temp) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[39m       [ True, False]])\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m element \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(element)\n\u001b[0;32m--> 890\u001b[0m \u001b[39mreturn\u001b[39;00m in1d(element, test_elements, assume_unique\u001b[39m=\u001b[39;49massume_unique,\n\u001b[1;32m    891\u001b[0m             invert\u001b[39m=\u001b[39;49minvert, kind\u001b[39m=\u001b[39;49mkind)\u001b[39m.\u001b[39mreshape(element\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:738\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# Otherwise use sorting\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m assume_unique:\n\u001b[0;32m--> 738\u001b[0m     ar1, rev_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(ar1, return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    739\u001b[0m     ar2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(ar2)\n\u001b[1;32m    741\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((ar1, ar2))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    330\u001b[0m optional_indices \u001b[39m=\u001b[39m return_index \u001b[39mor\u001b[39;00m return_inverse\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m optional_indices:\n\u001b[0;32m--> 333\u001b[0m     perm \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39;49margsort(kind\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmergesort\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m return_index \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mquicksort\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiawu/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:175: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed=58287\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action  [array([0.28852123], dtype=float32)]\n",
      "action  [array([-0.59269464], dtype=float32)]\n",
      "action  [array([0.50181675], dtype=float32)]\n",
      " self.episode  0\n",
      "action  [array([0.61323065], dtype=float32)]\n",
      "action  [array([0.80752575], dtype=float32)]\n",
      "action  [array([0.7324553], dtype=float32)]\n",
      " self.episode  1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(evaluation_time_episode):\n",
    "  done  = False\n",
    "  obs = env.reset()[0] \n",
    "  while not done:  # every step has same seed\n",
    "    action = [env.action_space.sample()]\n",
    "    print(\"action \", action)\n",
    "    obs, reward, terminated, done, info = env.step(action[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[473.9449194 ,  69.26051304,   4.06592848],\n",
       "        [528.57625633,  70.        ,   3.84625406],\n",
       "        [540.        ,  65.23814231,   4.98941154],\n",
       "        [528.18124652,  64.55583066,   5.        ],\n",
       "        [460.40459991,  54.67063218,   5.        ]],\n",
       "\n",
       "       [[540.        ,  59.83631993,   1.29984466],\n",
       "        [540.        ,  68.49595631,   2.26605854],\n",
       "        [506.78163171,  70.        ,   1.55509207],\n",
       "        [486.17304325,  70.        ,   1.        ],\n",
       "        [529.76484776,  70.        ,   1.        ]],\n",
       "\n",
       "       [[466.84106017,  70.        ,   2.00088892],\n",
       "        [350.69348479,  70.        ,   1.        ],\n",
       "        [300.        ,  70.        ,   1.07695407],\n",
       "        [406.66452885,  70.        ,   1.85820562],\n",
       "        [434.55042243,  64.75175202,   1.        ]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"/home/xiawu/RL4PT/results/PPO_Wed Sep 20 18:08:43 2023/test_result/3_ppo_action.npy\")\n",
    "np.load(\"/home/xiawu/RL4PT/results/PPO_Wed Sep 20 18:08:43 2023/test_result/3_ppo_toll.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start training with total steps:  120\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiawu/py38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 3, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 1\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./results/PPO_Wed Sep 20 17:45:22 2023/tensorboards/PPO/first_run_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiawu/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:175: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed=19147\u001b[0m\n",
      "  logger.warn(\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n",
      "/home/xiawu/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1115: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n",
      "  tollparameters = np.array([120(2*random.random()-1)+420, 10*(2*random.random()-1)+60, 2*(2*random.random()-1)+3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m CheckpointCallback(save_freq\u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m20\u001b[39m\u001b[39m*\u001b[39msimulation_day_num), save_path\u001b[39m=\u001b[39m(save_dir\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/logs/\u001b[39m\u001b[39m\"\u001b[39m), name_prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m PPO(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, learning_rate\u001b[39m=\u001b[39mlearning_rate, n_steps\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, \n\u001b[1;32m     12\u001b[0m             target_kl\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, n_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, gae_lambda\u001b[39m=\u001b[39m\u001b[39m0.97\u001b[39m,  ent_coef \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, clip_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m, \n\u001b[1;32m     13\u001b[0m             tensorboard_log\u001b[39m=\u001b[39m(save_dir\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/tensorboards/PPO/\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps \u001b[39m=\u001b[39;49m train_time_steps, tb_log_name\u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mfirst_run\u001b[39;49m\u001b[39m\"\u001b[39;49m, callback \u001b[39m=\u001b[39;49m checkpoint_callback)\n\u001b[1;32m     15\u001b[0m model\u001b[39m.\u001b[39msave(save_dir\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/logs/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(train_time_steps)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_steps\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mlearn)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[1;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    315\u001b[0m     )\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:178\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    176\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 178\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/normalize.py:81\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_vector_env:\n\u001b[1;32m     83\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(obs)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1117\u001b[0m, in \u001b[0;36mCommuteEnv_testing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom:\n\u001b[1;32m   1115\u001b[0m         tollparameters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m120\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mrandom\u001b[39m.\u001b[39mrandom()\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m420\u001b[39m, \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mrandom\u001b[39m.\u001b[39mrandom()\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m60\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mrandom\u001b[39m.\u001b[39mrandom()\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m])\n\u001b[0;32m-> 1117\u001b[0m state_encode, vehicle_information, market_price, pt_share_number, sw  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mRL_simulateOneday(tollparameters, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mday, state_aggravate) \u001b[39m# 5 days social welfare\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m observation \u001b[39m=\u001b[39m state_encode\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspace_shape)\n\u001b[1;32m   1120\u001b[0m AITT_daily \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# average system travel time per day is 34min, 31min\u001b[39;00m\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:799\u001b[0m, in \u001b[0;36mSimulation.RL_simulateOneday\u001b[0;34m(self, tollparameters, day, state_aggravate)\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mPTCs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mPTCs\n\u001b[1;32m    797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mAR\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mAR\n\u001b[0;32m--> 799\u001b[0m vehicle_information, time_list, Accumulation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMFD(day)\t\t\n\u001b[1;32m    800\u001b[0m actualArrival \u001b[39m=\u001b[39m vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_arr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m#update actual travel time\u001b[39;00m\n\u001b[1;32m    801\u001b[0m travel_time \u001b[39m=\u001b[39m  vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# daily travel time without PT\u001b[39;00m\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:707\u001b[0m, in \u001b[0;36mSimulation.MFD\u001b[0;34m(self, day)\u001b[0m\n\u001b[1;32m    703\u001b[0m Accumulation\u001b[39m.\u001b[39mappend(n)\n\u001b[1;32m    705\u001b[0m \u001b[39m# update the predicted arrival time for all cars which has entered the network\u001b[39;00m\n\u001b[1;32m    706\u001b[0m travel_started_vehicles \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere((S_Event_list_array[:, \u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m&\u001b[39m \n\u001b[0;32m--> 707\u001b[0m                                 (np\u001b[39m.\u001b[39;49misin(S_Event_list_array[:, \u001b[39m0\u001b[39;49m], vehicle_index)))\n\u001b[1;32m    708\u001b[0m temp \u001b[39m=\u001b[39m S_Event_list_array[(travel_started_vehicles)][:, \u001b[39m0\u001b[39m] \u001b[39m# get the vehicle index where the travel has been started\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39msize(temp) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[39m       [ True, False]])\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m element \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(element)\n\u001b[0;32m--> 890\u001b[0m \u001b[39mreturn\u001b[39;00m in1d(element, test_elements, assume_unique\u001b[39m=\u001b[39;49massume_unique,\n\u001b[1;32m    891\u001b[0m             invert\u001b[39m=\u001b[39;49minvert, kind\u001b[39m=\u001b[39;49mkind)\u001b[39m.\u001b[39mreshape(element\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:738\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# Otherwise use sorting\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m assume_unique:\n\u001b[0;32m--> 738\u001b[0m     ar1, rev_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(ar1, return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    739\u001b[0m     ar2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(ar2)\n\u001b[1;32m    741\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((ar1, ar2))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:354\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     mask[\u001b[39m1\u001b[39m:] \u001b[39m=\u001b[39m aux[\u001b[39m1\u001b[39m:] \u001b[39m!=\u001b[39m aux[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 354\u001b[0m ret \u001b[39m=\u001b[39m (aux[mask],)\n\u001b[1;32m    355\u001b[0m \u001b[39mif\u001b[39;00m return_index:\n\u001b[1;32m    356\u001b[0m     ret \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (perm[mask],)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\" Start training with total steps: \", train_time_steps,)\n",
    "env = gym.make(env_name, simulation_day_num = simulation_day_num, \n",
    "                save_episode_freq = save_episode_train, \n",
    "                train=True, save_dir = save_dir, space_shape=(4, int(12*60/5)),\n",
    "                One_dim = False, \n",
    "                Best_toll_initialization = False, \n",
    "                Reward_shifting = True\n",
    "                )\n",
    "env = NormalizeObservation(env)\n",
    "checkpoint_callback = CheckpointCallback(save_freq= int(20*simulation_day_num), save_path=(save_dir+\"/logs/\"), name_prefix=\"PPO\")\n",
    "model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, n_steps=4, verbose=1, batch_size=3, \n",
    "            target_kl=0.05, n_epochs=2, gae_lambda=0.97,  ent_coef = 0.5, clip_range=0.2, gamma=0.99, \n",
    "            tensorboard_log=(save_dir+\"/tensorboards/PPO/\"))\n",
    "model.learn(total_timesteps = train_time_steps, tb_log_name= \"first_run\", callback = checkpoint_callback)\n",
    "model.save(save_dir+\"/logs/\"+str(train_time_steps)+\"_steps\")\n",
    "print(model.learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiawu/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:175: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed=50237\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:  \u001b[39m# every step has same seed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         action \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(obs, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m         obs, reward, terminated, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinish testng!!!!!!!!!!!!!!!!!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/normalize.py:81\u001b[0m, in \u001b[0;36mNormalizeObservation.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     80\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and normalizes the observation.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     obs, rews, terminateds, truncateds, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_vector_env:\n\u001b[1;32m     83\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(obs)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step:\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:208\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    209\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m    210\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[1;32m    211\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:1133\u001b[0m, in \u001b[0;36mCommuteEnv_testing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoll_A \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m action[\u001b[39m2\u001b[39m]\n\u001b[1;32m   1131\u001b[0m     tollparameters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m120\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoll_mu\u001b[39m+\u001b[39m\u001b[39m420\u001b[39m, \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoll_sigma\u001b[39m+\u001b[39m\u001b[39m60\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoll_A\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m])\n\u001b[0;32m-> 1133\u001b[0m state_encode, vehicle_information, market_price, pt_share_number, sw  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mRL_simulateOneday(tollparameters, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mday, state_aggravate) \u001b[39m# 5 days social welfare\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m observation \u001b[39m=\u001b[39m state_encode\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspace_shape)\n\u001b[1;32m   1136\u001b[0m AITT_daily \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m# average system travel time per day is 34min, 31min\u001b[39;00m\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:813\u001b[0m, in \u001b[0;36mSimulation.RL_simulateOneday\u001b[0;34m(self, tollparameters, day, state_aggravate)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mPTCs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mPTCs\n\u001b[1;32m    811\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginalAtt[\u001b[39m'\u001b[39m\u001b[39mAR\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers\u001b[39m.\u001b[39mAR\n\u001b[0;32m--> 813\u001b[0m vehicle_information, time_list, Accumulation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMFD(day)\t\t\n\u001b[1;32m    814\u001b[0m actualArrival \u001b[39m=\u001b[39m vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_arr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m#update actual travel time\u001b[39;00m\n\u001b[1;32m    815\u001b[0m travel_time \u001b[39m=\u001b[39m  vehicle_information[\u001b[39m\"\u001b[39m\u001b[39mt_exp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m# daily travel time without PT\u001b[39;00m\n",
      "File \u001b[0;32m~/RL4PT/CustomEnvs/Custom_Envs/envs/test_env.py:718\u001b[0m, in \u001b[0;36mMFD\u001b[0;34m(self, day)\u001b[0m\n\u001b[1;32m    716\u001b[0m trip_len1 \u001b[39m=\u001b[39m vehicle_information[\u001b[39m'\u001b[39m\u001b[39mtrip_len(m)\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    717\u001b[0m trip_len1[vehicle_index] \u001b[39m=\u001b[39m trip_len1[vehicle_index] \u001b[39m-\u001b[39m V(n) \u001b[39m/\u001b[39m \u001b[39m60\u001b[39m \u001b[39m*\u001b[39m (t_ls[j] \u001b[39m-\u001b[39m t_ls[j \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]) \n\u001b[0;32m--> 718\u001b[0m vehicle_information[\u001b[39m'\u001b[39m\u001b[39mtrip_len(m)\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trip_len1\n\u001b[1;32m    719\u001b[0m n \u001b[39m=\u001b[39m n\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \n\u001b[1;32m    721\u001b[0m \u001b[39m# keep track of the accumulation\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[39mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[39m       [ True, False]])\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m element \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(element)\n\u001b[0;32m--> 890\u001b[0m \u001b[39mreturn\u001b[39;00m in1d(element, test_elements, assume_unique\u001b[39m=\u001b[39;49massume_unique,\n\u001b[1;32m    891\u001b[0m             invert\u001b[39m=\u001b[39;49minvert, kind\u001b[39m=\u001b[39;49mkind)\u001b[39m.\u001b[39mreshape(element\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:738\u001b[0m, in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m# Otherwise use sorting\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m assume_unique:\n\u001b[0;32m--> 738\u001b[0m     ar1, rev_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(ar1, return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    739\u001b[0m     ar2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(ar2)\n\u001b[1;32m    741\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((ar1, ar2))\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m~/py38/lib/python3.8/site-packages/numpy/lib/arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    330\u001b[0m optional_indices \u001b[39m=\u001b[39m return_index \u001b[39mor\u001b[39;00m return_inverse\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m optional_indices:\n\u001b[0;32m--> 333\u001b[0m     perm \u001b[39m=\u001b[39m ar\u001b[39m.\u001b[39;49margsort(kind\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmergesort\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m return_index \u001b[39melse\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mquicksort\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    334\u001b[0m     aux \u001b[39m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name, simulation_day_num = simulation_day_num, \n",
    "                save_episode_freq = save_episode_test, train=False, save_dir = save_dir, \n",
    "                space_shape=(4, int(12*60/5)),\n",
    "                One_dim = False, \n",
    "                Best_toll_initialization = False, \n",
    "                Reward_shifting = True\n",
    "                )\n",
    "env = NormalizeObservation(env)        \n",
    "for i in range(evaluation_time_episode):\n",
    "    done  = False\n",
    "    obs = env.reset()[0] \n",
    "    while not done:  # every step has same seed\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, done, info = env.step(action[0])\n",
    "print(\"finish testng!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
